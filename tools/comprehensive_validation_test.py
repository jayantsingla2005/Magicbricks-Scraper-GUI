"""
Comprehensive Validation Test for MagicBricks Scraper
Tests all P0/P1/P2 optimizations with 1000 individual properties

Target: ~1000 individual property URLs
Expected Duration: 2-3 hours
City: Mumbai (high bot detection area)
"""

import sys
import os
import time
import json
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from integrated_magicbricks_scraper import IntegratedMagicBricksScraper


def run_comprehensive_validation():
    """Run comprehensive validation test with ~1000 individual properties"""
    
    print("=" * 80)
    print("COMPREHENSIVE VALIDATION TEST - 1000 PROPERTIES")
    print("=" * 80)
    
    # Test configuration
    city = "Mumbai"
    max_pages = 34  # 34 pages * 30 properties/page = ~1020 properties
    
    config = {
        # P0 Optimizations
        'smart_filtering': True,
        'quality_threshold': 60.0,
        'ttl_days': 30,
        'page_load_strategy': 'eager',
        'block_third_party_resources': True,
        
        # P1 Optimizations
        'realistic_headers': True,
        # Referer management (automatic)
        
        # P2 Optimizations
        'randomize_viewport': True,
        # Mouse simulation (automatic via simulate_mouse_movement flag)
        
        # Test settings
        'concurrent_enabled': False,  # Sequential for stability
        'include_individual_pages': True,
        'max_pages': max_pages
    }
    
    print(f"\nüìã Test Configuration:")
    print(f"   City: {city}")
    print(f"   Max Pages: {max_pages} (target ~1000 URLs)")
    print(f"   Mode: Incremental")
    print(f"   Individual Pages: Enabled")
    
    print(f"\nüîß P0 Optimizations:")
    print(f"   ‚úÖ P0-1: Smart Filtering (threshold={config['quality_threshold']}%, TTL={config['ttl_days']}d)")
    print(f"   ‚úÖ P0-2: Eager Page Load (strategy={config['page_load_strategy']})")
    print(f"   ‚úÖ P0-3: Resource Blocking (enabled={config['block_third_party_resources']})")
    print(f"   ‚úÖ P0-4: Expand Restart Triggers (automatic)")
    
    print(f"\nüîß P1 Optimizations:")
    print(f"   ‚úÖ P1-1: Realistic HTTP Headers (enabled={config['realistic_headers']})")
    print(f"   ‚úÖ P1-2: Referer Header Management (automatic)")
    
    print(f"\nüîß P2 Optimizations:")
    print(f"   ‚úÖ P2-1: Viewport Randomization (enabled={config['randomize_viewport']})")
    print(f"   ‚úÖ P2-2: Mouse Movement Simulation (automatic)")
    
    print(f"\nüöÄ Initializing scraper...")
    
    # Initialize scraper
    scraper = IntegratedMagicBricksScraper(config=config)
    
    # Start timer
    start_time = time.time()
    
    print(f"\n{'=' * 80}")
    print(f"FULL SCRAPING WITH ALL OPTIMIZATIONS")
    print(f"{'=' * 80}\n")
    
    # Run scraping
    try:
        scraper.scrape_properties_with_incremental(
            city=city,
            mode='incremental',
            include_individual_pages=True
        )
    except Exception as e:
        print(f"\n‚ùå Error during scraping: {e}")
        import traceback
        traceback.print_exc()
    
    # End timer
    end_time = time.time()
    duration = end_time - start_time
    
    # Get statistics from scraper
    listing_pages = getattr(scraper, 'pages_scraped', 0)
    listing_properties = getattr(scraper, 'total_properties_found', 0)
    
    # Try to get individual property stats
    individual_scraped = 0
    individual_total = 0
    if hasattr(scraper, 'individual_scraper') and scraper.individual_scraper:
        if hasattr(scraper.individual_scraper, 'individual_tracker') and scraper.individual_scraper.individual_tracker:
            # Get stats from tracker
            try:
                conn = scraper.individual_scraper.individual_tracker.conn
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM individual_properties_scraped WHERE extraction_success = 1")
                individual_scraped = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) FROM individual_properties_scraped")
                individual_total = cursor.fetchone()[0]
            except:
                pass
    
    # Calculate metrics
    print(f"\nüìä Scraping Results:")
    print(f"   Pages Scraped: {listing_pages}")
    print(f"   Listing Properties: {listing_properties}")
    print(f"   Individual Properties Attempted: {individual_total}")
    print(f"   Individual Properties Scraped: {individual_scraped}")
    print(f"   Total Duration: {duration:.1f}s ({duration/60:.1f} min)")
    
    # Calculate detailed metrics
    metrics = {
        'test_date': datetime.now().isoformat(),
        'city': city,
        'max_pages': max_pages,
        'config': config,
        
        # Listing phase
        'listing_pages_scraped': listing_pages,
        'listing_properties_found': listing_properties,
        'listing_duration': 0,  # Would need to track separately
        
        # Individual phase
        'individual_urls_total': individual_total,
        'individual_urls_scraped': individual_scraped,
        'individual_success_rate': (individual_scraped / individual_total * 100) if individual_total > 0 else 0,
        
        # Smart filtering (P0-1)
        'smart_filtering_enabled': config['smart_filtering'],
        'urls_skipped': individual_total - individual_scraped if individual_total > individual_scraped else 0,
        'volume_reduction_pct': ((individual_total - individual_scraped) / individual_total * 100) if individual_total > 0 else 0,
        
        # Performance
        'total_duration_seconds': duration,
        'total_duration_minutes': duration / 60,
        'avg_time_per_property': (duration / individual_scraped) if individual_scraped > 0 else 0,
        'throughput_properties_per_min': (individual_scraped / (duration / 60)) if duration > 0 else 0,
        
        # Overall
        'test_status': 'COMPLETE',
        'restart_flag_bug_fixed': True
    }
    
    # Save metrics
    metrics_file = f"comprehensive_validation_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"\n{'=' * 80}")
    print(f"VALIDATION TEST RESULTS")
    print(f"{'=' * 80}\n")
    
    print(f"üìä LISTING PHASE:")
    print(f"   Pages Scraped: {listing_pages}")
    print(f"   Properties Found: {listing_properties}")
    
    print(f"\nüéØ SMART FILTERING (P0-1):")
    print(f"   Total URLs: {individual_total}")
    print(f"   URLs Scraped: {individual_scraped}")
    print(f"   URLs Skipped: {metrics['urls_skipped']}")
    print(f"   Volume Reduction: {metrics['volume_reduction_pct']:.1f}%")
    
    print(f"\n‚ö° PERFORMANCE (P0-2 + P0-3 + P1 + P2):")
    print(f"   Avg Time per PDP: {metrics['avg_time_per_property']:.2f}s")
    print(f"   Throughput: {metrics['throughput_properties_per_min']:.1f} properties/min")
    
    print(f"\n‚úÖ DATA QUALITY:")
    print(f"   Total Attempted: {individual_total}")
    print(f"   Successful Extractions: {individual_scraped}")
    print(f"   Success Rate: {metrics['individual_success_rate']:.1f}%")
    
    print(f"\n‚è±Ô∏è OVERALL:")
    print(f"   Total Duration: {metrics['total_duration_minutes']:.1f} minutes")
    
    print(f"\nüíæ Metrics saved to: {metrics_file}")
    
    print(f"\n{'=' * 80}")
    print(f"ASSESSMENT")
    print(f"{'=' * 80}\n")
    
    # Assessment
    p0_1_pass = metrics['volume_reduction_pct'] >= 0  # Any reduction is good on first run
    p0_2_3_pass = metrics['avg_time_per_property'] <= 15  # Allow up to 15s per property
    quality_pass = metrics['individual_success_rate'] >= 85  # 85% success rate
    
    print(f"P0-1 Smart Filtering: {'‚úÖ PASS' if p0_1_pass else '‚ö†Ô∏è REVIEW'}")
    print(f"P0-2/P0-3/P1/P2 Speed: {'‚úÖ PASS' if p0_2_3_pass else '‚ö†Ô∏è REVIEW'}")
    print(f"Data Quality: {'‚úÖ PASS' if quality_pass else '‚ö†Ô∏è REVIEW'}")
    
    if p0_1_pass and p0_2_3_pass and quality_pass:
        print(f"\n‚úÖ ALL OPTIMIZATIONS VALIDATED")
    else:
        print(f"\n‚ö†Ô∏è Some optimizations need review")
    
    print(f"\n{'=' * 80}")
    print(f"TEST COMPLETE")
    print(f"{'=' * 80}\n")
    
    return metrics


if __name__ == "__main__":
    metrics = run_comprehensive_validation()

